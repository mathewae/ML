# -*- coding: utf-8 -*-
"""Task_2_shared

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xoucb5_B8RkuyCN-9lW2pf0VRo7kb_FR
"""

# First, we import necessary libraries:
import numpy as np
import pandas as pd

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import GridSearchCV

from google.colab import drive
import sys
import os

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
path = 'MyDrive/Intro_to_ML/Task_2' 

drive.mount('/content/drive', force_remount=True)

drive_root = "/content/drive/MyDrive/Intro_to_ML/Task_2"

# Change to the directory
print("\nColab: Changing directory to ", drive_root)
# %cd $drive_root

def data_loading():
    """
    This function loads the training and test data, preprocesses it, removes the NaN values and interpolates the missing 
    data using imputation

    Parameters
    ----------
    Returns
    ----------
    X_train: matrix of floats, training input with features
    y_train: array of floats, training output with labels
    X_test: matrix of floats: dim = (100, ?), test input with features
    """

    # 0.A Load training data
    train_df = pd.read_csv("train.csv")
    train_column_names = train_df.columns.tolist() # save column names
    
    print("Training data:")
    print("Shape:", train_df.shape)
    print(train_df.head(2))
    print('\n')
    
    # 0.B Load test data
    test_df = pd.read_csv("test.csv")
    test_column_names = test_df.columns.tolist() # save column names

    print("Test data:")
    print(test_df.shape)
    print(test_df.head(2)) 


    ### TODO: Perform data preprocessing, imputation and extract X_train, y_train and X_test

    # 1. 
    # Encode the seasons in order to learn
    mapping = {'spring': 2, 'summer': 1, 'autumn': -1, 'winter': -2}
    train_df['season'] = train_df['season'].replace(mapping)
    test_df['season'] = test_df['season'].replace(mapping)


    # 2.A 
    # Imputation of training data. Using Multivariate feature imputation,
    # look at https://scikit-learn.org/stable/modules/impute.html 
    imp_train = IterativeImputer(max_iter=10, random_state=0)
    imp_train.fit(train_df)
    IterativeImputer(random_state=0)
    X_trans = imp_train.transform(train_df)
    # 2.B
    # Imputation of test data
    imp_test = IterativeImputer(max_iter=10, random_state=0)
    imp_test.fit(test_df)
    IterativeImputer(random_state=0)
    X_test = imp_test.transform(test_df)


    # 3.
    # Reshape data 
    X = pd.DataFrame(X_trans, columns=train_column_names)
    X_train = X.drop(['price_CHF'],axis=1)
    y_train = X['price_CHF']

    X_test = pd.DataFrame(X_test, columns=test_column_names)
    

    # 4. (optional)
    # Save repaired data if you want to compare, make graphs etc with original
    """
    data = pd.DataFrame(X_train) 
    data.to_csv('X_train_transformed.csv', index=False)
    data_y = pd.DataFrame(y_train) 
    data_y.to_csv('y_train_transformed.csv', index=False)
    """

    assert (X_train.shape[1] == X_test.shape[1]) and (X_train.shape[0] == y_train.shape[0]) and (X_test.shape[0] == 100), "Invalid data shape"
    return X_train, y_train, X_test

def modeling_and_prediction(X_train, y_train, X_test):
    """
    This function defines the model, fits training data and then does the prediction with the test data 

    Parameters
    ----------
    X_train: matrix of floats, training input with 10 features
    y_train: array of floats, training output
    X_test: matrix of floats: dim = (100, ?), test input with 10 features

    Returns
    ----------
    y_test: array of floats: dim = (100,), predictions on test set
    """

    #### TODO: Define the model and fit it using training data. Then, use test data to make predictions
    # 1.
    # parameters for gpr: choice of kernel, alpha, optimizer, normlize_y and more ...
    # parameters for RBF kernel: length_scale and length_scale_bounds
    # â€“> choosing RBF as kernel, only optimize length_scale and alpha using CV

    gpr = GaussianProcessRegressor()
    length_scale_range = np.logspace(-2, 2, 10) # range 0.01 to 100
    alpha_range = np.logspace(-2, 2, 10)        # range 0.0001 to 100


    # 2.
    # set up a GridSearchCV object with all parameter values to compare
    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html 
    param_grid = [{'kernel': [RBF(length_scale=l)],
                'alpha': alpha_range}
                for l in length_scale_range]
    """
    The GridSearchCV took a lot of time (8-9 min), and gave the optimal values:
    {'alpha': 0.21544346900318823, 'kernel': RBF(length_scale=12.9)}

    Possible to different values (and maybe fewer values to save time...)
    """
  
    # 3.
    # Perform the fit and prediction when having cv=5 folds
    grid_search = GridSearchCV(gpr, param_grid, cv=5)
    grid_search.fit(X_train, y_train)
    y_pred = grid_search.predict(X_test)

    print(grid_search.best_params_)

    assert y_pred.shape == (100,), "Invalid data shape"
    return y_pred

# Main function. You don't have to change this
if __name__ == "__main__":
    # Data loading
    X_train, y_train, X_test = data_loading()

    
    # The function retrieving optimal LR parameters
    y_pred=modeling_and_prediction(X_train, y_train, X_test)
    # Save results in the required format
    dt = pd.DataFrame(y_pred) 
    dt.columns = ['price_CHF']
    dt.to_csv('results.csv', index=False)
    print("\nResults file successfully generated!")
