# -*- coding: utf-8 -*-
"""final_task_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eDifoj3T4t76oux9QXQna0xytw2sXcTr
"""

import numpy as np
import pandas as pd

import torch 
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ExponentialLR

from sklearn.model_selection import train_test_split

from google.colab import drive

# Commented out IPython magic to ensure Python compatibility.

#Check if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Set to path
drive_root = "/content/drive/MyDrive/Intro_to_ML/final_task_4/"
drive.mount('/content/drive')
drive.mount('/content/drive', force_remount=True)

# Change to the directory
print("\nColab: Changing directory to ", drive_root)
# %cd $drive_root

def join_paths(end_path):
    # Set to path
    base_path = "/content/drive/MyDrive/Intro_to_ML/final_task_4/" # Make sure to end with '/'

    return base_path + end_path

## DATA LOADING ##

def load_data():

    pretrain_features = pd.read_csv(join_paths("pretrain_features.csv.zip"), index_col="Id", compression='zip').drop("smiles", axis=1).to_numpy()
    pretrain_labels = pd.read_csv(join_paths("pretrain_labels.csv.zip"), index_col="Id", compression='zip').to_numpy().squeeze(-1)
    train_features = pd.read_csv(join_paths("train_features.csv.zip"), index_col="Id", compression='zip').drop("smiles", axis=1).to_numpy()
    train_labels = pd.read_csv(join_paths("train_labels.csv.zip"), index_col="Id", compression='zip').to_numpy().squeeze(-1)
    test_features = pd.read_csv(join_paths("test_features.csv.zip"), index_col="Id", compression='zip').drop("smiles", axis=1).to_numpy()

    return pretrain_features, pretrain_labels, train_features, train_labels, test_features


def get_dataloader(features, labels = None, train = True, batch_size=256, shuffle=True):
    
    if train:
        dataset = TensorDataset(torch.from_numpy(features).type(torch.float), 
                                torch.from_numpy(labels).type(torch.float))
    else:
        dataset = TensorDataset(torch.from_numpy(features).type(torch.float))
    loader = DataLoader(dataset=dataset,
                        batch_size=batch_size,
                        shuffle=shuffle,
                        pin_memory=True)
    return loader

## MODELS ##

class Base_Network(nn.Module):
    def __init__(self, nr_features = 500):
        super(Base_Network,self).__init__()
        self.base = nn.Sequential(
            nn.Linear(1000, nr_features),
            nn.BatchNorm1d(nr_features),
            nn.ELU(),
            nn.Dropout(0.3),
            nn.Linear(nr_features,1)
            )

    def forward(self, x):
        x = self.base(x)
        return x
    

class End_Network(nn.Module):
    def __init__(self, base_network, nr_features = 500):
        super(End_Network,self).__init__()
        self.base_model = base_network
        self.additional_layers = nn.Sequential(
            nn.Linear(nr_features, 600),
            nn.BatchNorm1d(600),
            nn.ELU(),
            nn.Dropout(0.3),
            nn.Linear(600,256),
            nn.BatchNorm1d(256),
            nn.ELU(),
            nn.Dropout(0.3),
            nn.Linear(256,1)
        )

    def forward(self, x):
        base_model_output = self.base_model(x)
        x = self.additional_layers(base_model_output)
        return x

## BASE TRAINING ##

def train_base(data_loader, val_data_loader, nr_features):

    model = Base_Network(nr_features=nr_features).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()

    nr_epochs = 500
    best_loss = float('inf')
    best_val_loss = float('inf')
    patience = 20
    counter = 0

    for epoch in range(nr_epochs):
        model.train()
        run_loss = 0.0

        for [input, label] in data_loader:
            input = input.to(device)
            label = label.to(device)
            optimizer.zero_grad()
            out = model(input)
            loss = loss_fn(out.float().squeeze(), label.float())
            loss.backward()
            optimizer.step()
            run_loss += loss.item()

        epoch_loss = run_loss / len(data_loader)

        # Validate
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for [input, label] in val_data_loader:
                input = input.to(device)
                label = label.to(device)
                out = model(input)
                loss = loss_fn(out.float().squeeze(), label.float())
                val_loss += loss.item()

        val_loss = val_loss / len(val_data_loader)
        
        if val_loss < best_val_loss:
            best_loss = epoch_loss
            best_val_loss = val_loss
            best_model = model
            counter = 0
            print("Base model improved at epoch ", epoch+1, ", counter is at ", counter)

        else:
            counter +=1
        if (epoch + 1) % 10 == 0:
            print("\nEpoch ", epoch+1, "/", nr_epochs,": loss ", epoch_loss, " val loss: ", val_loss)

        # Check if early stopping criteria met
        if counter >= patience:
            print("\n\nEarly stopping triggered in epoch ", epoch+1, ". ")
            print("Best loss: ", best_loss, ", best validation loss: ", best_val_loss)
            break

    return best_model

## END TRAINING ##

def train_end(base_network, data_loader, nr_features, val_data_loader=None):

    model = End_Network(base_network, nr_features=nr_features).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)
    scheduler = ExponentialLR(optimizer=optimizer, gamma=0.995, verbose=False)
    loss_fn = nn.MSELoss()

    nr_epochs = 500
    best_loss = float('inf')
    best_val_loss = float('inf')
    patience = 50
    counter = 0

    for epoch in range(nr_epochs):
        model.train()
        run_loss = 0.0

        for [input, label] in data_loader:
            input = input.to(device)
            label = label.to(device)
            optimizer.zero_grad()
            out = model(input)
            loss = loss_fn(out.float().squeeze(), label.float())
            loss.backward()
            optimizer.step()
            run_loss += loss.item()

        epoch_loss = run_loss / len(data_loader)
        scheduler.step()

        if val_data_loader != None:
            # Validate
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for input, label in val_data_loader:
                    input = input.to(device)
                    label = label.to(device)
                    out = model(input)
                    loss = loss_fn(out.float().squeeze(), label.float())
                    val_loss += loss.item()

            val_loss = val_loss / len(val_data_loader)
            
            if val_loss < best_val_loss:
                best_loss = epoch_loss
                best_val_loss = val_loss
                best_model = model
                counter = 0
                print("End model improved at epoch ", epoch+1, ", counter is at ", counter)

            else:
                counter +=1
            
            if (epoch + 1) % 10 == 0:
                print("\nEpoch ", epoch+1, "/", nr_epochs,": loss ", epoch_loss, " val loss: ", val_loss)


            # Check if early stopping criteria met
            if counter >= patience:
                print("\n\nEarly stopping triggered in epoch ", epoch+1, ".")
                print("Best loss: ", best_loss, ", best validation loss: ", best_val_loss)
                break
        
        else:
            if  epoch_loss < best_loss:
                best_loss = epoch_loss
                best_model = model
                print("New best model found in epoch ", epoch+1)

            if (epoch + 1) % 10 == 0:
                print(f"\nEpoch {epoch+1} / {nr_epochs}:  loss {epoch_loss:.4f}. Optimal training - no validation set.")
    print("\n\nBest loss after all epochs: ", best_loss)
    
    return best_model

## MAIN ##

if __name__ == '__main__':

    ## 1. Load the data. (x_test to save index)
    pretrain_features, pretrain_labels, train_features, train_labels, test_features = load_data()
    x_test = pd.read_csv(join_paths("test_features.csv.zip"), index_col="Id", compression='zip').drop("smiles", axis=1)
    print("Data loaded.")

    # Create the training and validation sets
    pre_dat, pre_val_dat, pre_y, pre_val_y = train_test_split(pretrain_features, pretrain_labels, test_size=0.2, random_state=0)
    train_dat, train_val_dat, train_y, train_val_y = train_test_split(train_features, train_labels, test_size=0.2, random_state=0)

    # Create dataloaders
    pretrain_loader = get_dataloader(pre_dat, pre_y, train = True, batch_size=1000)
    pretrain_val_loader = get_dataloader(pre_val_dat, pre_val_y, train = True, batch_size=1000)

    train_loader = get_dataloader(train_dat, train_y, train = True, batch_size=10)
    train_val_loader = get_dataloader(train_val_dat, train_val_y, train = True, batch_size=10)
    train_loader_no_val = get_dataloader(train_features, train_labels, train=True, batch_size=10)

    test_loader = get_dataloader(test_features, train=False, batch_size=500, shuffle=False)
    print("DataLoaders created.")


    ## 2. Train the base network 
    print("\nTRAINING BASE NETWORK: \n")
    nr_features = 500
    base_network = train_base(pretrain_loader, pretrain_val_loader, nr_features=nr_features)
    
    print("\n\nBase network:", base_network)

    # Locking the weights of base network 
    for parameters in base_network.parameters():
        parameters.requires_grad = False
    
    # Retrieve base
    base = nn.Sequential(*list(base_network.base.children())[:-1])
    print("\nBase:")
    print(base)

    ## 3. Train the end network
    print("\nTRAINING END NETWORK: \n")

    # First model with validation set for tuning. Second for optimal fitting. 
    #model = train_end(base_network=base, data_loader=train_loader, val_data_loader=train_val_loader, nr_features=nr_features)
    model = train_end(base_network=base, data_loader=train_loader_no_val, val_data_loader=None, nr_features=nr_features)
    
    print("\nEnd network: ", model)

    ## 4. Predict labels for test features
    model.eval().to(device)
    y_pred = []

    with torch.no_grad():
        for [input] in test_loader:
            input = input.to(device)  
            out = model(input).cpu()
            y_pred.append(out.numpy())
    
    y_pred = np.vstack(y_pred).squeeze()

    assert y_pred.shape == (x_test.shape[0],)
    y_pred = pd.DataFrame({"y": y_pred}, index=x_test.index)
    y_pred.to_csv(join_paths("results.csv"), index_label="Id")
    print("\nPredictions saved, all done!")

