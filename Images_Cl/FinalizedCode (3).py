# -*- coding: utf-8 -*-
"""Individual Task 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RiV5dAgh1KZQUBP4z7cU3IeOl-g7gcTr
"""

# This serves as a template which will guide you through the implementation of this task.  It is advised
# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps

# First, we import necessary libraries:
import numpy as np
from torchvision import transforms
from torch.utils.data import DataLoader, TensorDataset
import os
import torch
from torchvision import transforms
import torchvision.datasets as datasets
import torch.nn as nn
from torch.nn.functional import normalize
import torch.nn.functional as F
from torchvision.models import resnet50, ResNet50_Weights
import matplotlib.pyplot as plt
import tensorflow as tf
from torchsummary import summary
from sklearn.preprocessing import normalize

from google.colab import drive
import sys
import os


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.cuda.is_available()

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
path = 'MyDrive/Task_3' 

drive.mount('/content/drive', force_remount=True)

drive_root = "/content/drive/MyDrive/Intro_to_ML/Task_3"

# Change to the directory
print("\nColab: Changing directory to ", drive_root)
# %cd $drive_root

def generate_embeddings():
    """
    Transform, resize and normalize the images and then use a pretrained model to extract 
    the embeddings.
    """
    # TODO: define a transform to pre-process the images. # Define the transformation to normalize the data. 
    mean = [0.5, 0.5, 0.5]
    std = [0.5, 0.5, 0.5]

    transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize the images to a fixed size
    transforms.ToTensor(),  # Convert the images to PyTorch tensors
    transforms.Normalize(mean, std)  # Normalize the pixel values
])


    train_dataset = datasets.ImageFolder(root="dataset", transform=transform)

    train_loader = DataLoader(dataset=train_dataset,
                              batch_size=64,
                              shuffle=False,
                              pin_memory=True, num_workers=2)
    

    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)

    # To remove last fully-connected layer:
    #print(model._modules)
    model = nn.Sequential(*list(model.children())[:-1])
    model.eval().to(device)

    embeddings = []
    embedding_size = 2048 #Size of penultimate Resnet50 Layer 

    num_images = len(train_dataset)
    embeddings = np.zeros((num_images, embedding_size))
    
    i = 0
    
    for img, label in train_dataset:
      img = img.unsqueeze(0)
      img = img.to(device)

    # Pass the image through the ResNet50 model and extract the features.
      with torch.no_grad():
        features = model(img.to(device))
        embeddings[i, :] = features.cpu().squeeze().numpy()
      i = i+1

    print(embeddings.shape)
    np.save('dataset/embeddings.npy', embeddings)

def get_data(file, train=True):
    """
    Load the triplets from the file and generate the features and labels.

    input: file: string, the path to the file containing the triplets
          train: boolean, whether the data is for training or testing

    output: X: numpy array, the features
            y: numpy array, the labels
    """
    
    triplets = []
    with open(file) as f:
        for line in f:
            triplets.append(line)

    # generate training data from triplets
    train_dataset = datasets.ImageFolder(root="dataset/",
                                         transform=None)
    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]
    embeddings = np.load('dataset/embeddings.npy')
    # TODO: Normalize the embeddings across the dataset. 
    mean = np.mean(embeddings, axis=1, keepdims=True)
    std = np.std(embeddings, axis=1,keepdims=True)
    normalized_data = (embeddings-mean)/std

    file_to_embedding = {}
    for i in range(len(filenames)):
        file_to_embedding[filenames[i]] = normalized_data[i]
    X = []
    y = []
    #use the individual embeddings to generate the features and labels for triplets
    #print(normalized_data.shape) # WORKING TILL HERE, PRINTING (10000,2048)
    
    for t in triplets:
        emb = [file_to_embedding[a] for a in t.split()]
        X.append(np.hstack([emb[0], emb[1], emb[2]]))
        y.append(1)
        # Generating negative samples (data augmentation)
        if train:
            X.append(np.hstack([emb[0], emb[2], emb[1]]))
            y.append(0)
    X = np.vstack(X)
    y = np.hstack(y)
    return X, y

def create_loader_from_np(X, y = None, train = True, batch_size=64, shuffle=True, num_workers = 2):
    """
    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.

    input: X: numpy array, the features
           y: numpy array, the labels
    
    output: loader: torch.data.util.DataLoader, the object containing the data
    """
    if train:
        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), 
                                torch.from_numpy(y).type(torch.long))
    else:
        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))
    loader = DataLoader(dataset=dataset,
                        batch_size=batch_size,
                        shuffle=shuffle,
                        pin_memory=True, num_workers=num_workers)
    return loader

class Net(nn.Module):
    """
    The model class, which defines our classifier.
    """
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(3*2048, 500),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(500, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.flatten(x)
        x = self.linear_relu_stack(x)      
        return x

def train_model(train_loader):
    """
    The training procedure of the model; it accepts the training data, defines the model 
    and then trains it.

    input: train_loader: torch.data.util.DataLoader, the object containing the training data
    
    output: model: torch.nn.Module, the trained model
    """
    model = Net()
    model.train()
    model.to(device)
    n_epochs = 10
    loss_fn = nn.BCELoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.005)
    size = len(train_loader.dataset)
    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part 
    # of the training data as a validation split. After each epoch, compute the loss on the 
    # validation split and print it out. This enables you to see how your model is performing 
    # on the validation data before submitting the results on the server. After choosing the 
    # best model, train it on the whole training data.
    for epoch in range(n_epochs):        
        for X, label in train_loader:
            label = label.unsqueeze(1)
            label = label.float()
            prediction = model(X.to(device))
            img_loss= loss_fn(prediction,label.to(device))

            optimizer.zero_grad()
            img_loss.backward()
            optimizer.step()
              
    return model

def test_model(model, loader):
    """
    The testing procedure of the model; it accepts the testing data and the trained model and 
    then tests the model on it.

    input: model: torch.nn.Module, the trained model
           loader: torch.data.util.DataLoader, the object containing the testing data
        
    output: None, the function saves the predictions to a results.txt file
    """
    model.eval().to(device)
    predictions = []
    # Iterate over the test data
    with torch.no_grad(): # We don't need to compute gradients for testing
        for [x_batch] in loader:
            x_batch= x_batch.to(device)
            predicted = model(x_batch)
            predicted = predicted.cpu().numpy()
            # Rounding the predictions to 0 or 1
            predicted[predicted >= 0.5] = 1
            predicted[predicted < 0.5] = 0
            predictions.append(predicted)
        predictions = np.vstack(predictions)
    np.savetxt("results.txt", predictions, fmt='%i')

if __name__ == '__main__':
    TRAIN_TRIPLETS = 'train_triplets.txt'
    TEST_TRIPLETS = 'test_triplets.txt'

    # generate embedding for each image in the dataset
    if(os.path.exists('dataset/embeddings.npy') == False):
      generate_embeddings()

    # load the training and testing data
    X, y = get_data('train_triplets.txt')
    X_test, _ = get_data(TEST_TRIPLETS, train=False)


    # Create data loaders for the training and testing data
    train_loader = create_loader_from_np(X, y, train = True, batch_size=64)
    test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)

    # define a model and train it
    model = train_model(train_loader)
    
    # test the model on the test data
    test_model(model, test_loader)
    print("Results saved to results.txt")